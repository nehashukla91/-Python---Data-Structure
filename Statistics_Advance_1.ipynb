{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nehashukla91/-Python---Data-Structure/blob/main/Statistics_Advance_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the properties of the F-distribution\n",
        "\n",
        "ANS--> The F-distribution, named after Sir Ronald Fisher, is a continuous probability distribution used in statistical hypothesis testing and regression analysis.\n",
        "\n",
        "Properties of the F-distribution:\n",
        "\n",
        "1. Right-skewed: The F-distribution is skewed to the right, meaning it has a longer tail on the right side.\n",
        "\n",
        "2. Non-symmetric: The distribution is not symmetric around its mean.\n",
        "\n",
        "3. Positive values only: The F-distribution only takes positive values.\n",
        "\n",
        "4. Two parameters: The F-distribution has two parameters: degrees of freedom (df1 and df2).\n",
        "\n",
        "5. Dependent on degrees of freedom: The shape of the distribution changes with df1 and df2.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "1. Mean: The mean of the F-distribution is df2 / (df2 - 2) for df2 > 2.\n",
        "\n",
        "2. Variance: The variance is 2 * (df2^2 * (df1 + df2 - 2)) / (df1 * (df2 - 2)^2 * (df2 - 4)).\n",
        "\n",
        "3. Mode: The mode is typically around 1.\n",
        "\n",
        "Types of F-distributions:\n",
        "\n",
        "1. F(1, df2): Used for simple linear regression.\n",
        "2. F(df1, df2): Used for multiple linear regression and analysis of variance (ANOVA).\n",
        "\n",
        "Common Applications:\n",
        "\n",
        "1. Hypothesis testing: F-tests for regression coefficients, ANOVA, and analysis of covariance (ANCOVA).\n",
        "2. Regression analysis: Testing significance of regression coefficients.\n",
        "3. Analysis of variance: Comparing means across multiple groups.\n",
        "\n",
        "Relationship to Other Distributions:\n",
        "\n",
        "1. Chi-squared distribution: The F-distribution is related to the chi-squared distribution.\n",
        "2. t-distribution: The square of a t-statistic follows an F-distribution."
      ],
      "metadata": {
        "id": "XtJ9-RlxtINA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "ANS--> The F-distribution is used in various statistical tests, primarily in:\n",
        "\n",
        "1. Analysis of Variance (ANOVA):\n",
        "\n",
        "- Tests equality of means across multiple groups.\n",
        "- F-statistic compares variance between groups (SSB) to variance within groups (SSW).\n",
        "- F-distribution is appropriate due to:\n",
        "    - Independence of SSB and SSW.\n",
        "    - Chi-squared distribution of SSB and SSW under normality.\n",
        "\n",
        "2. Regression Analysis:\n",
        "\n",
        "- Tests significance of regression coefficients.\n",
        "- F-statistic compares explained variance (RSS) to residual variance (RSE).\n",
        "- F-distribution is appropriate due to:\n",
        "    - Linearity and independence of residuals.\n",
        "    - Normality of residuals.\n",
        "\n",
        "3. Analysis of Covariance (ANCOVA):\n",
        "\n",
        "- Tests equality of means across multiple groups while controlling for covariates.\n",
        "- F-statistic compares variance between groups to variance within groups.\n",
        "- F-distribution is appropriate due to:\n",
        "    - Independence of SSB and SSW.\n",
        "    - Normality of residuals.\n",
        "\n",
        "4. Multiple Comparisons:\n",
        "\n",
        "- Tests differences between multiple group means.\n",
        "- F-statistic controls Type I error rate.\n",
        "- F-distribution is appropriate due to:\n",
        "    - Independence of comparisons.\n",
        "    - Normality of data.\n",
        "\n",
        "5. Time Series Analysis:\n",
        "\n",
        "- Tests significance of autoregressive (AR) or moving average (MA) coefficients.\n",
        "- F-statistic compares explained variance to residual variance.\n",
        "- F-distribution is appropriate due to:\n",
        "    - Linearity and independence of residuals.\n",
        "    - Normality of residuals.\n",
        "\n",
        "Why F-distribution is appropriate:\n",
        "\n",
        "1. Independence: F-distribution assumes independence of numerator and denominator.\n",
        "2. Normality: F-distribution assumes normality of residuals or data.\n",
        "3. Chi-squared distribution: F-distribution is related to chi-squared distribution.\n",
        "4. Ratio of variances: F-statistic compares variances, making F-distribution suitable.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "1. Normality of residuals or data.\n",
        "2. Independence of observations.\n",
        "3. Homoscedasticity (constant variance).\n",
        "4. Linearity (in regression analysis)."
      ],
      "metadata": {
        "id": "U3qcfYqltISf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?\n",
        "\n",
        "ANS--> Conducting an F-test to compare variances requires:\n",
        "\n",
        "Key Assumptions:\n",
        "\n",
        "1. Normality: Data from both populations should follow a normal distribution.\n",
        "2. Independence: Observations within and between groups should be independent.\n",
        "3. Homoscedasticity: Variances within each population should be constant.\n",
        "4. Random Sampling: Samples should be randomly selected from each population.\n",
        "5. Equal Subpopulation Variances (optional): If testing for equal variances, assume σ1 = σ2.\n",
        "\n",
        "Additional Considerations:\n",
        "\n",
        "1. No significant outliers: Outliers can inflate variance estimates.\n",
        "2. No significant skewness: Skewed data can affect variance estimates.\n",
        "3. No significant correlation: Correlated data can affect independence.\n",
        "\n",
        "F-Test Specific Assumptions:\n",
        "\n",
        "1. Ratio of Variances: F-test assumes the ratio of variances (σ1² / σ2²) follows an F-distribution.\n",
        "2. Degrees of Freedom: F-test requires calculation of degrees of freedom (df1, df2) for each population.\n",
        "\n",
        "Consequences of Violating Assumptions:\n",
        "\n",
        "1. Inaccurate p-values: Incorrect conclusions about variance differences.\n",
        "2. Reduced test power: Failure to detect significant differences.\n",
        "3. Increased Type I error: Incorrect rejection of null hypothesis.\n",
        "\n",
        "Alternatives if Assumptions are Violated:\n",
        "\n",
        "1. Non-parametric tests (e.g., Levene's test, Brown-Forsythe test)\n",
        "2. Transformations (e.g., logarithmic, square root)\n",
        "3. Robust statistical methods (e.g., trimmed means)"
      ],
      "metadata": {
        "id": "yFYsmeyCtIWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "ANS--> Purpose of ANOVA (Analysis of Variance):\n",
        "\n",
        "ANOVA is a statistical technique used to:\n",
        "\n",
        "1. Compare means of three or more groups.\n",
        "2. Determine if there are significant differences between group means.\n",
        "3. Analyze the relationship between a continuous outcome variable and one or more categorical predictor variables.\n",
        "\n",
        "Key Features of ANOVA:\n",
        "\n",
        "1. Compares multiple groups (3+).\n",
        "2. Assesses variance between and within groups.\n",
        "3. F-statistic measures the ratio of between-group variance to within-group variance.\n",
        "\n",
        "Comparison to t-test:\n",
        "\n",
        "t-test:\n",
        "\n",
        "1. Compares means of two groups.\n",
        "2. Assesses difference between two group means.\n",
        "3. t-statistic measures the difference between group means relative to standard error.\n",
        "\n",
        "Key differences:\n",
        "\n",
        "1. Number of groups: ANOVA (3+ groups) vs. t-test (2 groups).\n",
        "2. Variance analysis: ANOVA examines both between-group and within-group variance, while t-test focuses on difference between two means.\n",
        "3. Statistical power: ANOVA is more powerful for detecting differences among multiple groups.\n",
        "\n",
        "Types of ANOVA:\n",
        "\n",
        "1. One-way ANOVA: Compares means of three or more groups with one independent variable.\n",
        "2. Two-way ANOVA: Examines interactions between two independent variables.\n",
        "3. Repeated Measures ANOVA: Analyzes data from repeated measurements.\n",
        "\n",
        "Assumptions:\n",
        "\n",
        "1. Normality of residuals.\n",
        "2. Homoscedasticity (equal variances).\n",
        "3. Independence of observations."
      ],
      "metadata": {
        "id": "IcqhJSWLtIZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups.\n",
        "\n",
        "ANS--> Use one-way ANOVA when:\n",
        "\n",
        "1. Comparing means of three or more groups.\n",
        "2. Investigating the effect of a single categorical independent variable (factor) on a continuous outcome variable.\n",
        "3. Wanting to determine if there are significant differences between group means.\n",
        "\n",
        "Why prefer One-way ANOVA over Multiple t-tests:\n",
        "\n",
        "1. Control of Type I error rate: ANOVA protects against inflated Type I error rates (false positives) that occur when conducting multiple t-tests.\n",
        "\n",
        "2. Increased statistical power: ANOVA is more powerful for detecting differences among multiple groups.\n",
        "\n",
        "3. Simpler interpretation: ANOVA provides a single F-statistic and p-value, whereas multiple t-tests yield multiple p-values.\n",
        "\n",
        "4. Accounting for variance: ANOVA considers both between-group and within-group variance.\n",
        "\n",
        "Problems with Multiple t-tests:\n",
        "\n",
        "1. Inflated Type I error rate: Conducting multiple t-tests increases the likelihood of false positives.\n",
        "2. Multiple comparisons problem: Difficult to interpret and adjust for multiple p-values.\n",
        "3. Lack of consideration of variance: t-tests focus on pair-wise comparisons, neglecting overall variance.\n",
        "\n",
        "Additional benefits of One-way ANOVA:\n",
        "\n",
        "1. Post-hoc tests: Allows for pairwise comparisons after significant ANOVA results.\n",
        "2. Contrast analysis: Enables examination of specific group comparisons.\n",
        "3. Generalizability: ANOVA results are more generalizable to the population.\n",
        "\n",
        "Assumptions for One-way ANOVA:\n",
        "\n",
        "1. Normality of residuals.\n",
        "2. Homoscedasticity (equal variances).\n",
        "3. Independence of observations.\n",
        "\n",
        "Common applications:\n",
        "\n",
        "1. Comparing treatment outcomes.\n",
        "2. Analyzing survey responses.\n",
        "3. Evaluating differences in experimental conditions."
      ],
      "metadata": {
        "id": "Oi8PsxTStIhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "ANS--> Variance Partitioning in ANOVA:\n",
        "\n",
        "ANOVA partitions the total variance in the data into two components:\n",
        "\n",
        "1. Between-Group Variance (SSB): Measures the variation between group means.\n",
        "2. Within-Group Variance (SSW): Measures the variation within each group.\n",
        "\n",
        "Total Variance (SST): Sum of SSB and SSW.\n",
        "\n",
        "Mathematical Representation:\n",
        "\n",
        "SST = SSB + SSW\n",
        "\n",
        "Breaking Down the Components:\n",
        "\n",
        "1. SSB (Between-Group Variance):\n",
        "    - Calculated as: Σn_i(μ_i - μ)^2\n",
        "    - Measures the sum of squared differences between group means and the overall mean.\n",
        "    - Degrees of freedom: k-1 (where k is the number of groups)\n",
        "2. SSW (Within-Group Variance):\n",
        "    - Calculated as: ΣΣ(x_ij - μ_i)^2\n",
        "    - Measures the sum of squared differences between individual observations and their group means.\n",
        "    - Degrees of freedom: N-k (where N is the total sample size)\n",
        "\n",
        "F-Statistic Calculation:\n",
        "\n",
        "The F-statistic is calculated as the ratio of between-group variance to within-group variance:\n",
        "\n",
        "F = (MSB / MSW)\n",
        "\n",
        "Where:\n",
        "\n",
        "- MSB (Mean Square Between) = SSB / (k-1)\n",
        "- MSW (Mean Square Within) = SSW / (N-k)\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "- A large F-statistic indicates significant differences between group means (SSB dominates).\n",
        "- A small F-statistic indicates little difference between group means (SSW dominates).\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        "1. Mean Square (MS): Average squared deviation.\n",
        "2. Degrees of Freedom: Number of independent pieces of information used to calculate variance.\n",
        "\n",
        "ANOVA Table:\n",
        "\n",
        "| Source | SS | df | MS | F |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| Between Groups | SSB | k-1 | MSB | F = MSB/MSW |\n",
        "| Within Groups | SSW | N-k | MSW |  |\n",
        "| Total | SST | N-1 |  |  |\n"
      ],
      "metadata": {
        "id": "MLJC3YIMueAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "ANS--> Classical (Frequentist) Approach to ANOVA:\n",
        "\n",
        "1. Null Hypothesis Significance Testing (NHST): Tests whether the observed data support rejecting the null hypothesis.\n",
        "2. p-values: Measure the probability of observing the data (or more extreme) assuming the null hypothesis is true.\n",
        "3. α-level (e.g., 0.05): Arbitrary threshold for significance.\n",
        "4. Point estimates: Estimate population parameters (e.g., means) with a single value.\n",
        "\n",
        "Bayesian Approach to ANOVA:\n",
        "\n",
        "1. Probabilistic modeling: Updates probabilities based on data and prior knowledge.\n",
        "2. Bayes' theorem: Combines prior distributions with likelihood functions to estimate posterior distributions.\n",
        "3. Credible intervals: Quantify uncertainty around parameter estimates.\n",
        "4. Model comparison: Evaluates evidence for different models using Bayes factors.\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "Uncertainty Handling:\n",
        "\n",
        "1. Frequentist: p-values and confidence intervals provide indirect measures of uncertainty.\n",
        "2. Bayesian: Posterior distributions and credible intervals directly quantify uncertainty.\n",
        "\n",
        "Parameter Estimation:\n",
        "\n",
        "1. Frequentist: Point estimates (e.g., sample mean).\n",
        "2. Bayesian: Distributional estimates (e.g., posterior distribution of population mean).\n",
        "\n",
        "Hypothesis Testing:\n",
        "\n",
        "1. Frequentist: Null hypothesis testing with p-values.\n",
        "2. Bayesian: Model comparison using Bayes factors or posterior probabilities.\n",
        "\n",
        "Additional Bayesian Advantages:\n",
        "\n",
        "1. Incorporating prior knowledge: Bayesian methods can incorporate expert knowledge or previous research.\n",
        "2. Flexibility: Bayesian models can handle complex data structures and non-normal distributions.\n",
        "3. Interpretability: Bayesian results provide direct probability statements about parameters.\n",
        "\n",
        "Challenges and Limitations:\n",
        "\n",
        "1. Computational complexity: Bayesian methods can be computationally intensive.\n",
        "2. Prior sensitivity: Results may depend on the choice of prior distributions.\n",
        "3. Interpretation: Requires understanding of Bayesian statistics and probability theory.\n",
        "\n",
        "Software for Bayesian ANOVA:\n",
        "\n",
        "1. R (e.g., brms, rstanarm)\n",
        "2. Python (e.g., PyMC3, scikit-bayes)\n",
        "3. JAGS\n",
        "4. Stan\n"
      ],
      "metadata": {
        "id": "hZd_qrlwuvF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "* Profession A: [48, 52, 55, 60, 62'\n",
        "* Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
        "\n"
      ],
      "metadata": {
        "id": "PslP1nYAvDaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Incomes for each profession\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate variances\n",
        "var_a = np.var(profession_a, ddof=1)\n",
        "var_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "# Calculate F-statistic\n",
        "f_statistic = var_a / var_b\n",
        "\n",
        "# Calculate p-value (two-tailed test)\n",
        "dof_a = len(profession_a) - 1\n",
        "dof_b = len(profession_b) - 1\n",
        "p_value = 2 * min(stats.f.cdf(f_statistic, dof_a, dof_b), 1 - stats.f.cdf(f_statistic, dof_a, dof_b))\n",
        "\n",
        "# Output the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XChdNZI4z355",
        "outputId": "8aed6495-cd3a-46cf-dc48-dfd04a8ad880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        "* Region A: [160, 162, 165, 158, 164]\n",
        "* Region B: [172, 175, 170, 168, 174]\n",
        "* Region C: [180, 182, 179, 185, 183]\n",
        "* Task: Write Python code to perform the one-way ANOVA and interpret the result.\n",
        "* Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "Sg1z7uXtvrun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Heights for each region\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Output the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Unt0xbWyleW",
        "outputId": "2e0a762d-2caf-4df4-dcbc-f432a0419a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n"
          ]
        }
      ]
    }
  ]
}